{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22bcfe15",
   "metadata": {},
   "source": [
    "# Generate ESM embeddings using ESM-2 form Meta\n",
    "- Loads your .fasta file\n",
    "- Feeds each sequence into ESM-2\n",
    "- Extracts [CLS] token embedding (or mean pooled)\n",
    "- Saves to a .csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bab6b61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "## Import necessary libraries\n",
    "import torch\n",
    "import esm\n",
    "import os\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "from tqdm import tqdm\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c892f3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate ESM-2 embeddings for protein sequences\n",
    "def generate_esm2_embeddings(fasta_path, esm2_model_path, output_csv=\"../data/step4.1_esm_protein_embeddings.csv\"):\n",
    "    # Load pretrained ESM-2 model locally\n",
    "    print(\"Loading local ESM-2 model...\")\n",
    "    \n",
    "    # Fix for PyTorch 2.6+ weights_only default change\n",
    "    import torch.serialization\n",
    "    import argparse\n",
    "    torch.serialization.add_safe_globals([argparse.Namespace])\n",
    "    \n",
    "    try:\n",
    "        model, alphabet = esm.pretrained.load_model_and_alphabet_local(esm2_model_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load with weights_only=True, trying with weights_only=False...\")\n",
    "        # Monkey patch torch.load to use weights_only=False for ESM models\n",
    "        original_load = torch.load\n",
    "        def patched_load(*args, **kwargs):\n",
    "            kwargs['weights_only'] = False\n",
    "            return original_load(*args, **kwargs)\n",
    "        torch.load = patched_load\n",
    "        \n",
    "        model, alphabet = esm.pretrained.load_model_and_alphabet_local(esm2_model_path)\n",
    "        \n",
    "        # Restore original torch.load\n",
    "        torch.load = original_load\n",
    "    \n",
    "    batch_converter = alphabet.get_batch_converter()\n",
    "    model.eval()\n",
    "\n",
    "    # Get the number of layers from the model\n",
    "    num_layers = model.num_layers\n",
    "\n",
    "    # Read sequences\n",
    "    sequences = list(SeqIO.parse(fasta_path, \"fasta\"))\n",
    "    print(f\"Found {len(sequences)} protein sequences.\")\n",
    "\n",
    "    records = []\n",
    "\n",
    "    for record in tqdm(sequences, desc=\"Generating embeddings\"):\n",
    "        name = record.id\n",
    "        sequence = str(record.seq)\n",
    "\n",
    "        if len(sequence) > 4096:\n",
    "            print(f\"Skipping {name}: sequence too long for ESM-2\")\n",
    "            continue\n",
    "\n",
    "        batch_labels, batch_strs, batch_tokens = batch_converter([(name, sequence)])\n",
    "        with torch.no_grad():\n",
    "            results = model(batch_tokens, repr_layers=[num_layers], return_contacts=False)\n",
    "\n",
    "        token_representations = results[\"representations\"][num_layers]\n",
    "        cls_embedding = token_representations[0, 0, :].numpy()\n",
    "\n",
    "        records.append({\n",
    "            \"id\": name,\n",
    "            **{f\"feat_{i}\": cls_embedding[i] for i in range(len(cls_embedding))}\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Embeddings saved to '{output_csv}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06677876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading local ESM-2 model...\n",
      "Found 188 protein sequences.\n",
      "Found 188 protein sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:   4%|▎         | 7/188 [01:00<15:58,  5.30s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping P78527|CHEMBL3142|DNA-dependent: sequence too long for ESM-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 188/188 [42:16<00:00, 13.49s/it]   \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved to 'data/step4.1_esm_protein_embeddings.csv'\n"
     ]
    }
   ],
   "source": [
    "#Run\n",
    "if __name__ == \"__main__\":\n",
    "    fasta_path = \"../data/step3_kinase_target_sequences.fasta\"\n",
    "    esm2_model_path = r\"C:\\Users\\FEL_BA_01\\.cache\\torch\\hub\\checkpoints\\esm2_t33_650M_UR50D.pt\"\n",
    "    generate_esm2_embeddings(fasta_path, esm2_model_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
